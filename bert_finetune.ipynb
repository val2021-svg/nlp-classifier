{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "j2bJ42i95Nbn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import cuda\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7EgVgFPRHZ5E",
        "outputId": "8f317f9b-0597-4760-e754-f1b82c395d1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# Set device\n",
        "\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "\n",
        "# Initialize model and tokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=12, problem_type=\"multi_label_classification\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DJZWDeSnHZ5N"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "with open('train.json', 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Define label mapping\n",
        "label_mapping = {\n",
        "    \"Politics\": 0, \"Health\": 1, \"Finance\": 2, \"Travel\": 3, \"Food\": 4,\n",
        "    \"Education\": 5, \"Environment\": 6, \"Fashion\": 7, \"Science\": 8,\n",
        "    \"Sports\": 9, \"Technology\": 10, \"Entertainment\": 11\n",
        "}\n",
        "\n",
        "\n",
        "# Initialize lists for sentences and labels\n",
        "train_sentences = []\n",
        "test_sentences = []\n",
        "train_labels = []\n",
        "test_labels = []\n",
        "\n",
        "\n",
        "# Split into training and test sets per class\n",
        "for label in data.keys():\n",
        "    sentences = data[label]\n",
        "    labels = [label_mapping[label]] * len(sentences)\n",
        "\n",
        "    train_sents, test_sents, train_lbls, test_lbls = train_test_split(sentences, labels, test_size=0.1)\n",
        "\n",
        "    train_sentences.extend(train_sents)\n",
        "    test_sentences.extend(test_sents)\n",
        "    train_labels.extend(train_lbls)\n",
        "    test_labels.extend(test_lbls)\n",
        "\n",
        "\n",
        "def one_hot_encode(label, num_classes):\n",
        "    one_hot = [0.0] * num_classes\n",
        "    one_hot[label] = 1.0\n",
        "    return one_hot\n",
        "\n",
        "# One-hot encode labels\n",
        "num_classes = len(label_mapping)\n",
        "train_labels = [one_hot_encode(label, num_classes) for label in train_labels]\n",
        "test_labels = [one_hot_encode(label, num_classes) for label in test_labels]\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
        "\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "# Tokenize sentences\n",
        "train_inputs = tokenizer(train_sentences, truncation=True, padding=True, return_tensors=\"pt\")\n",
        "test_inputs = tokenizer(test_sentences, truncation=True, padding=True, return_tensors=\"pt\")\n",
        "\n",
        "# TextDataset\n",
        "train_dataset = TextDataset(train_inputs, train_labels)\n",
        "test_dataset = TextDataset(test_inputs, test_labels)\n",
        "\n",
        "# DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpbK7xMGV7bS",
        "outputId": "e3eb9e64-67e1-43c0-cc4b-cad1d318eece"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/100, Train Loss: 1.3451427221298218, Validation Loss: 0.7207010984420776\n",
            "Epoch 20/100, Train Loss: 0.4760288894176483, Validation Loss: 0.7035806775093079\n",
            "Epoch 30/100, Train Loss: 0.1962185651063919, Validation Loss: 0.6876175999641418\n",
            "Epoch 40/100, Train Loss: 0.0994560718536377, Validation Loss: 0.6883150339126587\n",
            "Epoch 50/100, Train Loss: 0.06522887200117111, Validation Loss: 0.6826522946357727\n",
            "Epoch 60/100, Train Loss: 0.04786122217774391, Validation Loss: 0.6806918382644653\n",
            "Epoch 70/100, Train Loss: 0.03901243209838867, Validation Loss: 0.6808144450187683\n",
            "Epoch 80/100, Train Loss: 0.030921983532607555, Validation Loss: 0.679891049861908\n",
            "Epoch 90/100, Train Loss: 0.026677953079342842, Validation Loss: 0.681160569190979\n",
            "Epoch 100/100, Train Loss: 0.024675155989825726, Validation Loss: 0.6852779984474182\n"
          ]
        }
      ],
      "source": [
        "# Model Training\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "epochs = 100\n",
        "\n",
        "for epoch in range(epochs):  # Number of epochs\n",
        "\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_accuracy = 0\n",
        "    i = 0\n",
        "    for batch in train_loader:\n",
        "        optim.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits  # Get the logits from the model output\n",
        "\n",
        "        _, predicted_labels = torch.max(logits, dim=1)\n",
        "\n",
        "\n",
        "        label_true = torch.argmax(labels, dim = 1)\n",
        "        correct = (predicted_labels == label_true).sum().item()\n",
        "\n",
        "        # Compute cross-entropy loss\n",
        "        loss_fn = torch.nn.CrossEntropyLoss()\n",
        "        loss = loss_fn(logits, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            val_loss += outputs.loss.item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(test_loader)\n",
        "    if (epoch+1)%10 == 0:\n",
        "      print(f'Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss}, Validation Loss: {avg_val_loss}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipx1V2CASkhR",
        "outputId": "beeecd76-0555-405f-f644-95eae1321b51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions saved to predictions_og_train.csv\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Test\n",
        "\n",
        "number_to_label = {value: key for key, value in label_mapping.items()}\n",
        "\n",
        "# Define class for testing\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, sentences, tokenizer, max_length=128):\n",
        "        self.sentences = sentences\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sentence = self.sentences[idx]\n",
        "        encoding = self.tokenizer(sentence, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
        "        return encoding\n",
        "\n",
        "# Load test sentences\n",
        "test_file = 'test_shuffle.txt'\n",
        "with open(test_file, 'r') as file:\n",
        "    test_sentences = file.readlines()\n",
        "\n",
        "# Create dataset\n",
        "test_dataset = TestDataset(test_sentences, tokenizer)\n",
        "\n",
        "# Create DataLoader\n",
        "batch_size = 4\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "# Model evaluation\n",
        "model.eval()\n",
        "predictions = []\n",
        "for batch_idx, batch in enumerate(test_loader):\n",
        "    input_ids = torch.squeeze(batch['input_ids'], dim=1).to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids = input_ids, attention_mask = attention_mask)\n",
        "\n",
        "    # Process outputs (if necessary)\n",
        "    logits = outputs.logits\n",
        "    _, predicted_labels = torch.max(logits, dim=1)\n",
        "\n",
        "    for label in predicted_labels.cpu().numpy():\n",
        "        category = number_to_label[label.item()]\n",
        "        predictions.append(category)\n",
        "\n",
        "# Create final DataFrame\n",
        "data = {\"ID\": range(len(predictions)), \"Label\": predictions}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Save to a CSV file\n",
        "output_file = \"predictions_og_train.csv\"\n",
        "df.to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"Predictions saved to {output_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYrN202HWdyI"
      },
      "source": [
        "No validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Vj_xIZBzWgNW"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load data\n",
        "with open('train.json', 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Define label mapping\n",
        "label_mapping = {\n",
        "    \"Politics\": 0, \"Health\": 1, \"Finance\": 2, \"Travel\": 3, \"Food\": 4,\n",
        "    \"Education\": 5, \"Environment\": 6, \"Fashion\": 7, \"Science\": 8,\n",
        "    \"Sports\": 9, \"Technology\": 10, \"Entertainment\": 11\n",
        "}\n",
        "\n",
        "\n",
        "# Initialize lists for sentences and labels\n",
        "train_sentences = []\n",
        "train_labels = []\n",
        "\n",
        "# Generate training data\n",
        "for label in data.keys():\n",
        "    sentences = data[label]\n",
        "    labels = [label_mapping[label]] * len(sentences)\n",
        "\n",
        "    train_sentences.extend(sentences)\n",
        "    train_labels.extend(labels)\n",
        "\n",
        "def one_hot_encode(label, num_classes):\n",
        "    one_hot = [0.0] * num_classes\n",
        "    one_hot[label] = 1.0\n",
        "    return one_hot\n",
        "\n",
        "# One-hot encode labels\n",
        "num_classes = len(label_mapping)\n",
        "train_labels = [one_hot_encode(label, num_classes) for label in train_labels]\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "# Tokenize sentences\n",
        "train_inputs = tokenizer(train_sentences, truncation=True, padding=True, return_tensors=\"pt\")\n",
        "\n",
        "# Create TextDataset for training\n",
        "train_dataset = TextDataset(train_inputs, train_labels)\n",
        "\n",
        "# Create DataLoader for training set\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G931tnbeWinU",
        "outputId": "62fc5d18-4b74-4d49-829c-ed85f07d396c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/100, Train Loss: 0.02659794936577479\n",
            "Epoch 20/100, Train Loss: 0.012459679506719112\n",
            "Epoch 30/100, Train Loss: 0.009110673641165098\n",
            "Epoch 40/100, Train Loss: 0.007444395373264949\n",
            "Epoch 50/100, Train Loss: 0.005695155511299769\n",
            "Epoch 60/100, Train Loss: 0.0043899849988520145\n",
            "Epoch 70/100, Train Loss: 0.0036366108494500318\n",
            "Epoch 80/100, Train Loss: 0.0034003082352379956\n",
            "Epoch 90/100, Train Loss: 0.002900610833118359\n",
            "Epoch 100/100, Train Loss: 0.00244701545064648\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Model Training\n",
        "\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "epochs = 100\n",
        "\n",
        "for epoch in range(epochs):  # Number of epochs\n",
        "\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_accuracy = 0\n",
        "    i = 0\n",
        "    for batch in train_loader:\n",
        "        optim.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits  # Get the logits from the model output\n",
        "\n",
        "        _, predicted_labels = torch.max(logits, dim=1)\n",
        "\n",
        "        label_true = torch.argmax(labels, dim = 1)\n",
        "        correct = (predicted_labels == label_true).sum().item()\n",
        "\n",
        "        # Compute loss\n",
        "        loss_fn = torch.nn.CrossEntropyLoss()\n",
        "        loss = loss_fn(logits, label_true)\n",
        "\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "    if (epoch+1)%10 == 0:\n",
        "      print(f'Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss}')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axGxH9U7WrmV",
        "outputId": "f951744b-fc5d-47ab-f242-a94191406ae0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions saved to predictions_augmented_train.csv\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Test\n",
        "\n",
        "number_to_label = {value: key for key, value in label_mapping.items()}\n",
        "\n",
        "# Define class for testing\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, sentences, tokenizer, max_length=128):\n",
        "        self.sentences = sentences\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sentence = self.sentences[idx]\n",
        "        encoding = self.tokenizer(sentence, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
        "        return encoding\n",
        "\n",
        "# Load test sentences\n",
        "test_file = 'test_shuffle.txt'\n",
        "with open(test_file, 'r') as file:\n",
        "    test_sentences = file.readlines()\n",
        "\n",
        "# Create dataset\n",
        "test_dataset = TestDataset(test_sentences, tokenizer)\n",
        "\n",
        "# Create DataLoader\n",
        "batch_size = 4\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "# Model evaluation\n",
        "model.eval()\n",
        "predictions = []\n",
        "for batch_idx, batch in enumerate(test_loader):\n",
        "    input_ids = torch.squeeze(batch['input_ids'], dim=1).to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids = input_ids, attention_mask = attention_mask)\n",
        "\n",
        "    # Process outputs (if necessary)\n",
        "    logits = outputs.logits\n",
        "    _, predicted_labels = torch.max(logits, dim=1)\n",
        "\n",
        "\n",
        "    for label in predicted_labels.cpu().numpy():\n",
        "        category = number_to_label[label.item()]\n",
        "        predictions.append(category)\n",
        "\n",
        "# Create final DataFrame\n",
        "data = {\"ID\": range(len(predictions)), \"Label\": predictions}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Save to a CSV file\n",
        "output_file = \"predictions_augmented_train.csv\"\n",
        "df.to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"Predictions saved to {output_file}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
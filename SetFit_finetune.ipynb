{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceType": "datasetVersion",
          "sourceId": 8133322,
          "datasetId": 4773162,
          "databundleVersionId": 8253352
        },
        {
          "sourceType": "datasetVersion",
          "sourceId": 8084649,
          "datasetId": 4772280,
          "databundleVersionId": 8200955
        },
        {
          "sourceType": "datasetVersion",
          "sourceId": 8031300,
          "datasetId": 4733852,
          "databundleVersionId": 8144267
        }
      ],
      "dockerImageVersionId": 30684,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'nlp-comp:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4733852%2F8031300%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240416%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240416T090221Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D73f8c18607d33bec368520e685d4a082c72cd325c528c243a26c69a08da1ed8f343cdada8ba3f098277d5256fddc04a7c8d80f9305308a33c7174f7bdf8df082d0a65504d7e104119ca5a3c9a1f5fc58c0d985d9e2b8da6546046e56dadd2db295ef2338c36667e7f9e23c466fce91da861547a9d1e0756822f4cea5073f7937184ad70d16a60aa4d1fdae1a1f15452b6e3c28459c1f010a8b0d7377efef6dcf5e55ebb886770bfcf2e4545ca039fe2b9a827b8c316173a1419bb163b590cac8ee38378a1f841fec8d567eb25251821ea5f3bee5b40dd2afbe402a5ee8bfd823f007067dadc09c3c97e8950abcf4d0a568d17f241f29f83ecf8b8dc23e2ce83b,nlp-augaugmented:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4772280%2F8084649%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240416%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240416T090221Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D17d62f42e9c36b879cc2352bd1ad2ceb45591c87c6968f86ad84ce965bd6dade46572c4bee80f54bbb9f3d2b7aab28cbaa091b0e8edb521410a8483ab41292daf95d1bf227f4223eed2bbd03bc18e2567b678b3d43f843f36e3df4b0fba90d52b0f2a1223da76701525877593da2c149492e18741aca8ab33adecc5291208e2c3ab9e590b0a0887428147f19d1eee3e6e920213951c6efcd67811d5595ead83d1148db4edf3f0f92eae42a8bdb0f6c2385175d77f5f6e5347c082dd2c65f76d6080f67a783a8a1e71301ca53bd11d38966b79f901c7a2d21880cc555cda2454f56094db7ffae20cdb8387e7b9d195110a7eba155d97e0cb22afdc779e506d93c,nlp-aug-1:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4773162%2F8133322%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240416%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240416T090221Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Dddfe7e2bb4173720b7df95ef95ce64db7e995bafbbd5c54f17700edfe5bdbf9db3f7bd77c70fafe064a9f183e823baab0fa041f0d003ab746b1dc5a3630c37c816e73255e3b8abfe80d91b225bae0cbca791db6e0528e27b5b1e0ea02682a4790f4b2ae3fd4a1ae12c9e3338a98c9d315fdc90dc50a73d91a97994123876f1c8a0f9913ebdcb47e764ef8ac77cb2d35b30b433deeaabef21db82301100a4472d401df90bcd8389f5ef93d7d3a99f534a782c8d97e99d89cd3f14f7e7a46509dad81a6bc19f87d9aaa9376dbd5c1a4f656b1b36176c8ff5f4cdcc6258aed3c8bdeab5e19b4cbdeafaf19f7c8da0665b21a9714707af46912818a626f41cca3901'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "FJ9_k0oCJJwQ"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset kaggle [link](https://kaggle.com/datasets/073e4e1d58a55b6f93cd62ab184277e2ba0ac229662d9f99155c461c2deab66e)"
      ],
      "metadata": {
        "id": "uedXwqYKJOJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install SetFit\n",
        "!pip3 install sklearn\n",
        "!pip3 install transformers\n",
        "!pip3 install sentence-transformers"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-16T08:52:06.122258Z",
          "iopub.execute_input": "2024-04-16T08:52:06.12304Z",
          "iopub.status.idle": "2024-04-16T08:52:47.89614Z",
          "shell.execute_reply.started": "2024-04-16T08:52:06.123004Z",
          "shell.execute_reply": "2024-04-16T08:52:47.894903Z"
        },
        "trusted": true,
        "id": "YB6vQyZ8JJwT",
        "outputId": "3c423c9b-a9ff-4627-e399-ede55fdff514"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Requirement already satisfied: SetFit in /opt/conda/lib/python3.10/site-packages (1.0.3)\nRequirement already satisfied: datasets>=2.3.0 in /opt/conda/lib/python3.10/site-packages (from SetFit) (2.18.0)\nRequirement already satisfied: sentence-transformers>=2.2.1 in /opt/conda/lib/python3.10/site-packages (from SetFit) (2.6.1)\nRequirement already satisfied: evaluate>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from SetFit) (0.4.1)\nRequirement already satisfied: huggingface-hub>=0.13.0 in /opt/conda/lib/python3.10/site-packages (from SetFit) (0.22.2)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from SetFit) (1.2.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from SetFit) (21.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.3.0->SetFit) (3.13.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.3.0->SetFit) (1.26.4)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.3.0->SetFit) (15.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets>=2.3.0->SetFit) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.3.0->SetFit) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets>=2.3.0->SetFit) (2.1.4)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.3.0->SetFit) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.3.0->SetFit) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets>=2.3.0->SetFit) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets>=2.3.0->SetFit) (0.70.16)\nRequirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets>=2.3.0->SetFit) (2024.2.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.3.0->SetFit) (3.9.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.3.0->SetFit) (6.0.1)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from evaluate>=0.3.0->SetFit) (0.18.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.13.0->SetFit) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->SetFit) (3.1.1)\nRequirement already satisfied: transformers<5.0.0,>=4.32.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers>=2.2.1->SetFit) (4.39.3)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers>=2.2.1->SetFit) (2.1.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers>=2.2.1->SetFit) (1.11.4)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers>=2.2.1->SetFit) (9.5.0)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->SetFit) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->SetFit) (3.2.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.3.0->SetFit) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.3.0->SetFit) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.3.0->SetFit) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.3.0->SetFit) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.3.0->SetFit) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.3.0->SetFit) (4.0.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets>=2.3.0->SetFit) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets>=2.3.0->SetFit) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets>=2.3.0->SetFit) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets>=2.3.0->SetFit) (2024.2.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=2.2.1->SetFit) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=2.2.1->SetFit) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=2.2.1->SetFit) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers>=2.2.1->SetFit) (2023.12.25)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers>=2.2.1->SetFit) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers>=2.2.1->SetFit) (0.4.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.3.0->SetFit) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.3.0->SetFit) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.3.0->SetFit) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.3.0->SetFit) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.2.1->SetFit) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers>=2.2.1->SetFit) (1.3.0)\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Collecting sklearn\n  Using cached sklearn-0.0.post12.tar.gz (2.6 kB)\n  Preparing metadata (setup.py) ... \u001b[?25lerror\n  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n  \n  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n  \u001b[31m╰─>\u001b[0m \u001b[31m[15 lines of output]\u001b[0m\n  \u001b[31m   \u001b[0m The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n  \u001b[31m   \u001b[0m rather than 'sklearn' for pip commands.\n  \u001b[31m   \u001b[0m \n  \u001b[31m   \u001b[0m Here is how to fix this error in the main use cases:\n  \u001b[31m   \u001b[0m - use 'pip install scikit-learn' rather than 'pip install sklearn'\n  \u001b[31m   \u001b[0m - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n  \u001b[31m   \u001b[0m   (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n  \u001b[31m   \u001b[0m - if the 'sklearn' package is used by one of your dependencies,\n  \u001b[31m   \u001b[0m   it would be great if you take some time to track which package uses\n  \u001b[31m   \u001b[0m   'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n  \u001b[31m   \u001b[0m - as a last resort, set the environment variable\n  \u001b[31m   \u001b[0m   SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n  \u001b[31m   \u001b[0m \n  \u001b[31m   \u001b[0m More information is available at\n  \u001b[31m   \u001b[0m https://github.com/scikit-learn/sklearn-pypi-package\n  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n  \n  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n\n\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n\u001b[31m╰─>\u001b[0m See above for output.\n\n\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n\u001b[1;36mhint\u001b[0m: See above for details.\n\u001b[?25h",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.39.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.22.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.2)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Requirement already satisfied: sentence-transformers in /opt/conda/lib/python3.10/site-packages (2.6.1)\nRequirement already satisfied: transformers<5.0.0,>=4.32.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.39.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.66.1)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (2.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.26.4)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.11.4)\nRequirement already satisfied: huggingface-hub>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.22.2)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (9.5.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2024.2.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (2023.12.25)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.4.2)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.2.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.15.1->sentence-transformers) (3.1.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_JqnPJvHomA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
        "from torch import cuda\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.functional import one_hot\n",
        "import torch\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from datasets import load_metric\n",
        "from datasets import Dataset\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-16T08:52:47.898574Z",
          "iopub.execute_input": "2024-04-16T08:52:47.898929Z",
          "iopub.status.idle": "2024-04-16T08:52:47.910493Z",
          "shell.execute_reply.started": "2024-04-16T08:52:47.898899Z",
          "shell.execute_reply": "2024-04-16T08:52:47.909419Z"
        },
        "trusted": true,
        "id": "wKRBylMjJJwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize nltk objects\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "import nltk\n",
        "import subprocess\n",
        "\n",
        "# Download and unzip wordnet\n",
        "try:\n",
        "    nltk.data.find('wordnet.zip')\n",
        "except:\n",
        "    nltk.download('wordnet', download_dir='/kaggle/working/')\n",
        "    command = \"unzip /kaggle/working/corpora/wordnet.zip -d /kaggle/working/corpora\"\n",
        "    subprocess.run(command.split())\n",
        "    nltk.data.path.append('/kaggle/working/')\n",
        "\n",
        "# Now you can import the NLTK resources as usual\n",
        "from nltk.corpus import wordnet"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-16T08:52:47.912063Z",
          "iopub.execute_input": "2024-04-16T08:52:47.9124Z",
          "iopub.status.idle": "2024-04-16T08:52:47.955995Z",
          "shell.execute_reply.started": "2024-04-16T08:52:47.912375Z",
          "shell.execute_reply": "2024-04-16T08:52:47.95506Z"
        },
        "trusted": true,
        "id": "fUJ7k90jJJwU",
        "outputId": "8f1d3cec-43ce-4c7c-f33d-ca8cb82c6c62"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package wordnet to /kaggle/working/...\n[nltk_data]   Package wordnet is already up-to-date!\nArchive:  /kaggle/working/corpora/wordnet.zip\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "replace /kaggle/working/corpora/wordnet/lexnames? [y]es, [n]o, [A]ll, [N]one, [r]ename:  NULL\n(EOF or read error, treating as \"[N]one\" ...)\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    # Lowercase the text\n",
        "    text = text.lower()\n",
        "    # Remove non-alphanumeric characters\n",
        "    text = re.sub(r'\\W', ' ', text)\n",
        "    # Tokenize text\n",
        "    tokens = word_tokenize(text)\n",
        "    # Remove stopwords and lemmatize\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words and len(word) > 1]\n",
        "    return ' '.join(tokens)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-16T08:52:47.958581Z",
          "iopub.execute_input": "2024-04-16T08:52:47.95961Z",
          "iopub.status.idle": "2024-04-16T08:52:47.966626Z",
          "shell.execute_reply.started": "2024-04-16T08:52:47.959572Z",
          "shell.execute_reply": "2024-04-16T08:52:47.965531Z"
        },
        "trusted": true,
        "id": "zAqaDbiqJJwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/kaggle/input/nlp-augaugmented/train_ds_augmented2.json', 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "label_mapping = {\n",
        "    \"Politics\": 0,\n",
        "    \"Health\": 1,\n",
        "    \"Finance\": 2,\n",
        "    \"Travel\": 3,\n",
        "    \"Food\": 4,\n",
        "    \"Education\": 5,\n",
        "    \"Environment\": 6,\n",
        "    \"Fashion\": 7,\n",
        "    \"Science\": 8,\n",
        "    \"Sports\": 9,\n",
        "    \"Technology\": 10,\n",
        "    \"Entertainment\": 11\n",
        "}\n",
        "\n",
        "texts = []\n",
        "labels = []\n",
        "\n",
        "# Process each item\n",
        "for label, sentences in data.items():\n",
        "    if label in label_mapping:\n",
        "        for sentence in sentences:\n",
        "            #texts.append(preprocess_textsentence))\n",
        "            texts.append(sentence)\n",
        "            labels.append(label_mapping[label])\n",
        "    else:\n",
        "        print(f\"Warning: Label '{label}' not found in label mapping.\")\n",
        "\n",
        "        # Create a Dataset object from the processed data\n",
        "ds = Dataset.from_dict({\n",
        "    \"text\": texts,\n",
        "    \"label\": labels\n",
        "})\n",
        "\n",
        "# Print the dataset to verify\n",
        "print(ds)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-16T08:52:47.967936Z",
          "iopub.execute_input": "2024-04-16T08:52:47.968269Z",
          "iopub.status.idle": "2024-04-16T08:52:48.002459Z",
          "shell.execute_reply.started": "2024-04-16T08:52:47.968237Z",
          "shell.execute_reply": "2024-04-16T08:52:48.00137Z"
        },
        "trusted": true,
        "id": "wj1HmPNHJJwV",
        "outputId": "093c1e2d-251f-4791-8aac-47c2d5399bc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Dataset({\n    features: ['text', 'label'],\n    num_rows: 1254\n})\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds = ds.shuffle(seed=42)  # Seed for reproducibility\n",
        "\n",
        "# Step 2: Split the dataset into train and test sets\n",
        "split_ds = ds.train_test_split(test_size=0.2)  # 10% for testing, 90% for training\n",
        "\n",
        "# Accessing the split datasets\n",
        "train_dataset = split_ds['train']\n",
        "test_dataset = split_ds['test']"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-16T08:52:48.003984Z",
          "iopub.execute_input": "2024-04-16T08:52:48.004372Z",
          "iopub.status.idle": "2024-04-16T08:52:48.023684Z",
          "shell.execute_reply.started": "2024-04-16T08:52:48.004335Z",
          "shell.execute_reply": "2024-04-16T08:52:48.022282Z"
        },
        "trusted": true,
        "id": "IRnZntCCJJwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from setfit import SetFitModel, SetFitTrainer\n",
        "from datasets import Dataset\n",
        "from sentence_transformers.losses import CosineSimilarityLoss\n",
        "\n",
        "model = SetFitModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "trainer = SetFitTrainer(\n",
        "    model=model,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    loss_class=CosineSimilarityLoss,\n",
        "    metric=\"accuracy\",\n",
        "    batch_size=1024,\n",
        "    num_iterations = 40,\n",
        "    num_epochs = 10\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "print(trainer.evaluate())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-16T08:59:33.687286Z",
          "iopub.execute_input": "2024-04-16T08:59:33.688278Z"
        },
        "trusted": true,
        "id": "p_VO4hg8JJwV",
        "outputId": "9e4a0e05-f7e1-4ffb-e793-38f861a31f38",
        "colab": {
          "referenced_widgets": [
            "9e3a122228ab455eaf7229e6ba6e9910"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n/tmp/ipykernel_453/4212682733.py:7: DeprecationWarning: `SetFitTrainer` has been deprecated and will be removed in v2.0.0 of SetFit. Please use `Trainer` instead.\n  trainer = SetFitTrainer(\n/opt/conda/lib/python3.10/site-packages/ipywidgets/widgets/widget.py:503: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  self.comm = Comm(**args)\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/1003 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9e3a122228ab455eaf7229e6ba6e9910"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "***** Running training *****\n  Num unique pairs = 80240\n  Batch size = 1024\n  Num epochs = 10\n  Total optimization steps = 790\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='94' max='790' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 94/790 02:21 < 17:51, 0.65 it/s, Epoch 1.18/0]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "{'accuracy': 0.8909090909090909}\n",
        "40 int 10 epochs nlp-aug-1 train_ds_augmented.json"
      ],
      "metadata": {
        "id": "xDarxNPqJJwW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input = [\"Campus Sutra Men's Sports Jersey T-Shirt Cool-Gear: Our Proprietary Moisture Management technology. Helps to absorb and evaporate sweat quickly. Keeps you Cool & Dry. Ultra-Fresh: Fabrics treated with Ultra-Fresh Antimicrobial Technology. Ultra-Fresh is a trademark of (TRA) Inc, Ontario, Canada. Keeps you odour free.\"]\n",
        "output = model(input)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-16T08:52:56.939929Z",
          "iopub.status.idle": "2024-04-16T08:52:56.940471Z",
          "shell.execute_reply.started": "2024-04-16T08:52:56.940186Z",
          "shell.execute_reply": "2024-04-16T08:52:56.94021Z"
        },
        "trusted": true,
        "id": "ly_m0d8KJJwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_texts_and_classify(file_path):\n",
        "    texts = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        lines = file.readlines()\n",
        "        for line in tqdm(lines, desc=\"Classifying texts\", position=0, leave=True):\n",
        "            text_to_classify = line.strip()\n",
        "            if text_to_classify:  # Ensuring the line is not empty\n",
        "                # Assuming get_prompt prepares the text for classification\n",
        "                # and that you have a tokenizer and model ready for use\n",
        "                texts.append(text_to_classify)\n",
        "    return texts"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-16T08:52:56.941814Z",
          "iopub.status.idle": "2024-04-16T08:52:56.94232Z",
          "shell.execute_reply.started": "2024-04-16T08:52:56.94205Z",
          "shell.execute_reply": "2024-04-16T08:52:56.942071Z"
        },
        "trusted": true,
        "id": "a4b1NYQvJJwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def labels_to_text(labels, label_mapping):\n",
        "    # Reverse the label_mapping dictionary to map numbers to category names\n",
        "\n",
        "    reverse_mapping = {value: key for key, value in label_mapping.items()}\n",
        "\n",
        "    # Convert list of numbers to list of text using the reversed mapping\n",
        "    text_labels = [reverse_mapping[label] for label in labels if label in reverse_mapping]\n",
        "\n",
        "    return text_labels"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-16T08:52:56.944552Z",
          "iopub.status.idle": "2024-04-16T08:52:56.945357Z",
          "shell.execute_reply.started": "2024-04-16T08:52:56.945093Z",
          "shell.execute_reply": "2024-04-16T08:52:56.945115Z"
        },
        "trusted": true,
        "id": "pr_siyZ5JJwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_file_path = '/kaggle/input/nlp-comp/test_shuffle.txt'\n",
        "# Convert to text labels\n",
        "label_mapping = {\n",
        "    \"Politics\": 0,\n",
        "    \"Health\": 1,\n",
        "    \"Finance\": 2,\n",
        "    \"Travel\": 3,\n",
        "    \"Food\": 4,\n",
        "    \"Education\": 5,\n",
        "    \"Environment\": 6,\n",
        "    \"Fashion\": 7,\n",
        "    \"Science\": 8,\n",
        "    \"Sports\": 9,\n",
        "    \"Technology\": 10,\n",
        "    \"Entertainment\": 11\n",
        "}\n",
        "texts = read_texts_and_classify(input_file_path)\n",
        "results = model(texts).cpu().numpy()\n",
        "converted_labels = labels_to_text(results, label_mapping)\n",
        "print(converted_labels)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-16T08:52:56.947107Z",
          "iopub.status.idle": "2024-04-16T08:52:56.947631Z",
          "shell.execute_reply.started": "2024-04-16T08:52:56.947353Z",
          "shell.execute_reply": "2024-04-16T08:52:56.947375Z"
        },
        "trusted": true,
        "id": "NRHesT6gJJwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a DataFrame with 'label' column\n",
        "df = pd.DataFrame({'Label': converted_labels})\n",
        "\n",
        "# Writing DataFrame to a CSV file\n",
        "df.to_csv('teste.csv', index_label='ID')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-16T08:52:56.949118Z",
          "iopub.status.idle": "2024-04-16T08:52:56.949659Z",
          "shell.execute_reply.started": "2024-04-16T08:52:56.949372Z",
          "shell.execute_reply": "2024-04-16T08:52:56.949393Z"
        },
        "trusted": true,
        "id": "R0cuuIXrJJwX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}